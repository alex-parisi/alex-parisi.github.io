<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Alex Parisi - atparisi.com</title>
    <script src="shader.js" type="module"></script>
    <script src="utils.js" type="modeule"></script>
    <script src="webgl-demo.js" type="module"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="webgl-demo.css"> 
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css">
  </head>
  <body>
    <div class="divcanvas">
      <canvas id="glcanvas"></canvas>
      <div id="backgroundEnable" class="container-fluid py-1 shadow-lg text-center rounded-4 animate__animated animate__fadeIn animate__slow center" style="position: absolute; width: 10%; top: 0%; left: 45%; z-index: 2;">
        <label class="form-check-label">
          <input class="form-check-input" type="checkbox" value="1" id="enable_btn" aria-checked="true" checked/>
          Enable Background
        </label>
      </div>
      <div id="wrapper" style="display: table;">
        <div id="the_biz" class="container-fluid py-3 px-3 shadow-lg text-center rounded-4 animate__animated animate__fadeIn animate__slow" style="position: absolute; width: 20%; top: 10%; left: 40%; display: table-cell; transition: transform 1s cubic-bezier(0.85, 0, 0.15, 1); height: 60%;">
          <h1>Alex Parisi</h1>
          <h4>Software Engineer</h4>
          <hr>
          <div class="row gap-3 p-3 align-items-center">
            <a id="experience_button" class="btn btn-outline-dark me-3 shadow animate__animated animate__flipInX" style="animation-delay:1000ms; font-size:1.5em;" role="button"><i class="bi bi-person-rolodex" style="font-size:1em"></i> Experience</a>
            <a id="education_button" class="btn btn-outline-dark me-3 shadow animate__animated animate__flipInX" style="animation-delay:1100ms; font-size:1.5em;" role="button"><i class="bi bi-mortarboard-fill" style="font-size:1em"></i> Education</a>
            <a id="projects_button" class="btn btn-outline-dark me-3 shadow animate__animated animate__flipInX" style="animation-delay:1200ms; font-size:1.5em;" role="button"><i class="bi bi-clipboard2-data-fill" style="font-size:1em"></i> Projects</a>
            <a id="skills_button" class="btn btn-outline-dark me-3 shadow animate__animated animate__flipInX" style="animation-delay:1300ms; font-size:1.5em;" role="button"><i class="bi bi-layout-text-window" style="font-size:1em"></i> Skills</a>
            <a id="patents_button" class="btn btn-outline-dark me-3 shadow animate__animated animate__flipInX" style="animation-delay:1400ms; font-size:1.5em;" role="button"><i class="bi bi-award-fill" style="font-size:1em"></i> Patents</a>
            <a id="misc_button" class="btn btn-outline-dark me-3 shadow animate__animated animate__flipInX" style="animation-delay:1500ms; font-size:1.5em;" role="button"><i class="bi bi-columns-gap" style="font-size:1em"></i> Misc</a>
          </div>
        </div>
        <div id="experience_deetz" class="container-fluid py-3 px-3 shadow-lg text-center rounded-4 animate__animated animate__fadeIn animate__delay-1s" style="position: absolute; width: 40%; top: 10%; left: 42.5%; display: table-cell;">
          <h2><i class="bi bi-person-rolodex" style="font-size:1em"></i> Experience</h2>
          <hr>
          <div id="carouselExperience" class="carousel slide carousel-dark">
            <div class="carousel-indicators">
              <button type="button" data-bs-target="#carouselExperience" data-bs-slide-to="0" class="active" aria-current="true" aria-label="Slide 1"></button>
              <button type="button" data-bs-target="#carouselExperience" data-bs-slide-to="1" aria-label="Slide 2"></button>
              <button type="button" data-bs-target="#carouselExperience" data-bs-slide-to="2" aria-label="Slide 3"></button>
            </div>          
            <div class="carousel-inner">
              <div class="carousel-item active">
                <div class="container-fluid py-3 px-3 text-center rounded-4 overflow-hidden" style="top:10%; bottom: 10%;">
                  <div class="row align-items-center">
                    <div class="col-8">
                      <h3 id="h31" align="left"><b>DSP Software Engineer</b></h4>
                    </div>
                    <div class="col">
                      <h6 id="h61" align="right"><em>Mar. 2022 - Present</em></h6>
                    </div>
                  </div>
                  <div class="row align-items-center">
                    <div class="col-8">
                      <h5 id="h51" align="left"><u>Mikucare</u></h6>
                    </div>
                    <div class="col">
                      <h6 id="h62" align="right"><em>Woodbridge, NJ</em></h6>
                    </div>
                  </div>
                  <hr>
                  <div id="miku_exp" class="container-fluid overflow-auto m-3" style="height: 250px;">
                    <p align="justify">Member of a team of engineers responsible for developing high throughput and low latency C++17 code for a smart baby monitor.</p>
                    <p align="justify">Leading the development of new algorithm features utilizing sensor fusion and computer vision techniques to identify the sleep, respiration, and movement patterns of over 30,000 infants in a fast-paced startup environment.</p>
                    <p align="justify">Designed a multi-threaded and highly-scalable analysis model that processed over a terabyte of customer data to verify algorithm performance and evaluate the statistics of the radar and camera sensors for the development of new features.</p>
                    <p align="justify">Analyzed the accuracy and performance of seizure and heartbeat detection algorithms in Python and debugged customer issues.</p>
                    <p align="justify">Collaborating with other engineers, product managers, and stakeholders to understand requirements and deliver cutting-edge, high-quality software solutions utilizing software best practices and an Agile methodology.</p>
                  </div>
                </div>
              </div>
              <div class="carousel-item">
                <div class="container-fluid py-3 px-3 text-center rounded-4 overflow-hidden" style="top:10%; bottom: 10%;">
                  <div class="row align-items-center">
                    <div class="col-8">
                      <h3 id="h32" align="left"><b>Software Engineer</b></h4>
                    </div>
                    <div class="col">
                      <h6 id="h63" align="right"><em>Nov. 2019 - Mar. 2022</em></h6>
                    </div>
                  </div>
                  <div class="row align-items-center">
                    <div class="col-8">
                      <h5 id="h52" align="left"><u>Peraton Labs</u></h6>
                    </div>
                    <div class="col">
                      <h6 id="h64" align="right"><em>Picatinny Arsenal, NJ</em></h6>
                    </div>
                  </div>
                  <hr>
                  <div id="peraton_exp" class="container-fluid overflow-auto m-3" style="height: 250px;">
                    <p align="justify">Member of an Agile team of engineers responsible for developing and maintaining the real-time object tracking and trajectory estimation software for a Counter-Unmanned-Aircraft-System (C-UAS) program at the US Army CCDC-AC: Weapons and Software Engineering Center in C++17.</p>
                    <p align="justify">Collaborated with large teams composed of industry experts to deliver high-performance and high-throughput software running on a custom Linux kernel, which interacts with several other systems and a proprietary weapons platform.</p>
                  </div>
                </div>
              </div>
              <div class="carousel-item">
                <div class="container-fluid py-3 px-3 text-center rounded-4 overflow-hidden" style="top:10%; bottom: 10%;">
                  <div class="row align-items-center">
                    <div class="col-8">
                      <h3 id="h33" align="left"><b>Firmware Engineer</b></h4>
                    </div>
                    <div id="h65" class="col">
                      <h6 align="right"><em>Jan. 2018 - Nov. 2019</em></h6>
                    </div>
                  </div>
                  <div class="row align-items-center">
                    <div class="col-8">
                      <h5 id="h53" align="left"><u>Crestron Electronics</u></h6>
                    </div>
                    <div class="col">
                      <h6 id="h66" align="right"><em>Rockleigh, NJ</em></h6>
                    </div>
                  </div>
                  <hr>
                  <div id="crestron_exp" class="container-fluid overflow-auto m-3" style="height: 250px;">
                    <p align="justify">Wrote and interacted with bare-metal and Real-Time-Operating-System (RTOS) C firmware, specifically relating to audio processing algorithms like line/acoustic echo cancellation, delay-and-sum beamformers, and fixed/adaptive filter design.</p>
                    <p align="justify">Modeled algorithms and product design feasibility in MATLAB, and participated in the full cycle of product development for the UC-SB1-CAM soundbar, one of Crestron's best-selling products.</p>
                  </div>
                </div>
              </div>
            </div>
            <button class="carousel-control-prev" type="button" data-bs-target="#carouselExperience" data-bs-slide="prev">
              <span class="carousel-control-prev-icon" aria-hidden="true"></span>
              <span class="visually-hidden">Previous</span>
            </button>
            <button class="carousel-control-next" type="button" data-bs-target="#carouselExperience" data-bs-slide="next">
              <span class="carousel-control-next-icon" aria-hidden="true"></span>
              <span class="visually-hidden">Next</span>
            </button>
          </div>
        </div>
        <div id="education_deetz" class="container-fluid py-3 px-3 shadow-lg text-center rounded-4 animate__animated animate__fadeIn animate__delay-1s" style="position: absolute; width: 40%; top: 10%; left: 42.5%; display: table-cell;">
          <h2><i class="bi bi-mortarboard-fill" style="font-size:1em"></i> Education</h2>
          <hr>
          <div class="row">
            <div class="col">
              <h4 id="h41" align="left">Georgia Institute of Technology</h4>
              <h6 id="h67" align="left"><em>&emsp;M.S. in Electrical and Computer Engineering</em></h6>
              <h6 id="h68" align="left"><em>&emsp;Concentration in Digital Signal Processing (DSP)</em></h6>
            </div>
            <div class="col">
              <h6 id="h69" align="right"><u>2016 - 2017</u></h6>
            </div>
          </div>
          <div class="row">
            <div class="col">
              <h4 id="h42" align="left">Manhattan College</h4>
              <h6 id="h610" align="left"><em>&emsp;B.S. in Computer Engineering</em></h6>
              <h6 id="h611" align="left"><em>&emsp;Minor in Mathematics</em></h6>
            </div>
            <div class="col">
              <h6 id="h612" align="right"><u>2012 - 2016</u></h6>
            </div>
          </div>
          <hr>
          <h4 id="h43" align="left">Relevant Projects & Papers:</h5>
          <div id="carouselEducation" class="carousel slide carousel-dark">
            <div class="carousel-indicators">
              <button type="button" data-bs-target="#carouselEducation" data-bs-slide-to="0" class="active" aria-current="true" aria-label="Slide 1"></button>
              <button type="button" data-bs-target="#carouselEducation" data-bs-slide-to="1" aria-label="Slide 2"></button>
              <button type="button" data-bs-target="#carouselEducation" data-bs-slide-to="2" aria-label="Slide 3"></button>
              <button type="button" data-bs-target="#carouselEducation" data-bs-slide-to="3" aria-label="Slide 4"></button>
              <button type="button" data-bs-target="#carouselEducation" data-bs-slide-to="4" aria-label="Slide 5"></button>
              <button type="button" data-bs-target="#carouselEducation" data-bs-slide-to="5" aria-label="Slide 6"></button>
            </div> 
            <div class="carousel-inner">
              <div class="carousel-item active">
                <div id="paper1_cont" class="container-fluid py-3 px-3 text-center rounded-4 overflow-hidden" style="height: 200px;">
                  <h6 id="h613" align="left"><u>Heteronymous Ambiguity Resolution</u></h6>
                  <div id="paper1" class="container-fluid overflow-auto m-1" style="height: 50%;">
                    <p align="justify">This project used a combination of an N-Gram tagger, a Bayesian classifier, and a decision tree to determine the correct pronunciation of heteronyms (words that are spelled the same but pronounced differently, i.e. "lead") for text-to-speech implementations.</p>
                    <a class="btn btn-outline-dark me-3 shadow" href="https://github.com/alex-parisi/Heteronymous-Ambiguity-Resolution" role="button" target="_blank"><i class="bi bi-github"></i><br>Link</a>
                  </div>
                </div>
              </div>
              <div class="carousel-item">
                <div id="paper2_cont" class="container-fluid py-3 px-3 text-center rounded-4 overflow-hidden" style="height: 200px;">
                  <h6 id="h614" align="left"><u>Identity Detection via Handwriting Analysis</u></h6>
                  <div id="paper2" class="container-fluid overflow-auto m-1" style="height: 50%;">
                    <p align="justify">This project used the Discrete Curvelet Transform to extract statistical features from handwritten characters in order to establish the identity of the writer. A profile was constructed for each user, and samples of their handwriting were analyzed. New writing samples could then be analyzed and have their statistical features compared to the character of each user profile.</p>
                    <a class="btn btn-outline-dark me-3 shadow" href="https://github.com/alex-parisi/Identity-Detection-via-Handwriting-Analysis" role="button" target="_blank"><i class="bi bi-github"></i><br>Link</a>
                  </div>
                </div>
              </div>
              <div class="carousel-item">
                <div id="paper3_cont" class="container-fluid py-3 px-3 text-center rounded-4 overflow-hidden" style="height: 200px;">
                  <h6 id="h615" align="left"><u>EEG Beamforming</u></h6>
                  <div id="paper3" class="container-fluid overflow-auto m-1" style="height: 50%;">
                    <p align="justify">The focus of this project was to determine how beamforming algorithms and techniques can be applied to electroencephalographic measurements to optimize source detection and resolve sources with a high spatial correlation. Applying beamforming to EEG's can provide a significantly higher amount of spatial resolution than standard dipole modeling, and as a result can provide more accurate measurements and produce more meaningful data.</p>
                    <a class="btn btn-outline-dark me-3 shadow" href="https://github.com/alex-parisi/EEG-Beamforming" role="button" target="_blank"><i class="bi bi-github"></i><br>Link</a>
                  </div>
                </div>
              </div>
              <div class="carousel-item">
                <div id="paper4_cont" class="container-fluid py-3 px-3 text-center rounded-4 overflow-hidden" style="height: 200px;">
                  <h6 id="h616" align="left"><u>Phased Vocoder</u></h6>
                  <div id="paper4" class="container-fluid overflow-auto m-1" style="height: 50%;">
                    <p align="justify">The purpose of this project was to design a phase vocoder capable of pitch-shifting without time-stretching, time-stretching without pitch-shifting, and pitch-shifting while time-stretching speech signals.</p>
                    <a class="btn btn-outline-dark me-3 shadow" href="https://github.com/alex-parisi/Phased-Vocoder" role="button" target="_blank"><i class="bi bi-github"></i><br>Link</a>
                  </div>
                </div>
              </div>
              <div class="carousel-item">
                <div id="paper5_cont" class="container-fluid py-3 px-3 text-center rounded-4 overflow-hidden" style="height: 200px;">
                  <h6 id="h617" align="left"><u>Optimizing Brain Tumor Images</u></h6>
                  <div id="paper5" class="container-fluid overflow-auto m-1" style="height: 50%;">
                    <p align="justify">Using brain tumor images acquired from an optical fluorescence microscope imaging system employing a 488 nanometer Argon laser, this project used the Otsu algorithm to highlight cancerous brain tissue segments, facilitating faster and more accurate diagnoses of brain cancer while providing a surgeon with more accurate images to reduce the possibility of healthy tissue removal during brain cancer operations.</p>
                    <a class="btn btn-outline-dark me-3 shadow" href="https://github.com/alex-parisi/Optimizing-Brain-Tumor-Images" role="button" target="_blank"><i class="bi bi-github"></i><br>Link</a>
                  </div>
                </div>
              </div>
              <div class="carousel-item">
                <div id="paper6_cont" class="container-fluid py-3 px-3 text-center rounded-4 overflow-hidden" style="height: 200px;">
                  <h6 id="h618" align="left"><u>Free Space Optical Data Transmission</u></h6>
                  <div id="paper6" class="container-fluid overflow-auto m-1" style="height: 50%;">
                    <p align="justify">NASA's USIP (Undergraduate Student Instrument Project) SFRO (Student Flight Research Opportunity) Program: this project focused on the design of a small CubeSat satellite that would be launched into orbit around Earth. My team and I would then send signals via an 852 nanometer laser to test the feasibility of replacing radio frequency communications between satellites with free-space optical data transmission using high-intensity lasers. Unfortunately the project did not receive funding.</p>
                    <a class="btn btn-outline-dark me-3 shadow" href="https://github.com/alex-parisi/Free-Space-Optical-Data-Transmission" role="button" target="_blank"><i class="bi bi-github"></i><br>Link</a>
                  </div>
                </div>
              </div>
            </div>
            <button class="carousel-control-prev" type="button" data-bs-target="#carouselEducation" data-bs-slide="prev">
              <span class="carousel-control-prev-icon" aria-hidden="true"></span>
              <span class="visually-hidden">Previous</span>
            </button>
            <button class="carousel-control-next" type="button" data-bs-target="#carouselEducation" data-bs-slide="next">
              <span class="carousel-control-next-icon" aria-hidden="true"></span>
              <span class="visually-hidden">Next</span>
            </button>
          </div>
        </div>
        <div id="projects_deetz" class="container-fluid py-3 px-3 shadow-lg text-center rounded-4 animate__animated animate__fadeIn animate__delay-1s" style="position: absolute; width: 20%; top: 10%; left: 17.5%; display: table-cell; height: 60%;">
          <h2><i class="bi bi-clipboard2-data-fill" style="font-size:1em"></i> Projects</h2>
          <hr>
          <div class="row gap-3 p-3 align-items-center">
            <a id="projectsMainMenu_button" class="btn btn-sm btn-outline-dark me-3 shadow animate__animated animate__flipInX" style="animation-delay:1000ms; font-size:1.5em;" role="button"><i class="bi bi-menu-button-wide" style="font-size:1em"></i> Main Menu</a>
            <a id="projectDeetz1_button" class="btn btn-sm btn-outline-dark me-3 shadow animate__animated animate__flipInX " style="animation-delay:1100ms; font-size:1.em;" role="button"> ESRGAN Super-Resolution</a>
            <a id="projectDeetz2_button" class="btn btn-sm btn-outline-dark me-3 shadow animate__animated animate__flipInX" style="animation-delay:1200ms; font-size:1.em;" role="button"> Style Transfer</a>
            <a id="projectDeetz3_button" class="btn btn-sm btn-outline-dark me-3 shadow animate__animated animate__flipInX" style="animation-delay:1300ms; font-size:1.em;" role="button"> DCGAN</a>
            <a id="projectDeetz4_button" class="btn btn-sm btn-outline-dark me-3 shadow animate__animated animate__flipInX" style="animation-delay:1400ms; font-size:1.em;" role="button"> Django Website</a>
            <a id="projectDeetz5_button" class="btn btn-sm btn-outline-dark me-3 shadow animate__animated animate__flipInX" style="animation-delay:1500ms; font-size:1.em;" role="button"> OpenGL Renderer</a>
          </div>
        </div>
        <div id="skills_deetz" class="container-fluid py-3 px-3 shadow-lg text-center rounded-4 animate__animated animate__fadeIn animate__delay-1s" style="position: absolute; width: 40%; top: 10%; left: 42.5%; display: table-cell;">
          <h2><i class="bi bi-layout-text-window" style="font-size:1em"></i> Skills</h2>
          <hr>
          <h4 align="left"><em>Programming:</em></h4>
          <p>Python, C++, MATLAB, C, JavaScript, HTML & CSS, Markdown, SQL, Unix, Bash, AWS</p>
          <hr>
          <h4 align="left"><em>Software & Packages:</em></h4>
          <p>OpenGL, WebGL, Tensorflow/Keras, PyTorch, numpy, scipy, pandas, matplotlib, git, Latex</p>
          <hr>
          <h4 align="left"><em>General:</em></h4>
          <p>Firmware Integration, Statistic Analysis, Low-Latency Code, Multi-Threaded Systems, C++ STL</p>
        </div>
        <div id="patents_deetz" class="container-fluid py-3 px-3 shadow-lg text-center rounded-4 animate__animated animate__fadeIn animate__delay-1s" style="position: absolute; width: 40%; top: 10%; left: 42.5%; display: table-cell;">
          <h2><i class="bi bi-award-fill" style="font-size:1em"></i> Patents</h2>
          <hr>
          <h5 align="left">Adaptive Beamforming Microphone Metadata Transmission to Coordinate Acoustic Echo Cancellation in an Audio Conferencing System</h5>
          <a href="https://patents.google.com/patent/US10854216B2/" class="link-dark link-offset-2 link-underline-opacity-25 link-underline-opacity-100-hover" target="_blank"><p align="left">US Patent: US10854216B2</p></a>
          <hr>
        </div>
        <div id="misc_deetz" class="container-fluid py-3 px-3 shadow-lg text-center rounded-4 animate__animated animate__fadeIn animate__delay-1s" style="position: absolute; width: 20%; top: 10%; left: 17.5%; display: table-cell;">
          <h2><i class="bi bi-columns-gap" style="font-size:1em"></i> Miscellaneous</h2>
          <hr>
          <div class="row gap-3 p-3 align-items-center">
            <a id="miscMainMenu_button" class="btn btn-sm btn-outline-dark me-3 shadow animate__animated animate__flipInX" style="animation-delay:1000ms; font-size:1.5em;" role="button"><i class="bi bi-menu-button-wide" style="font-size:1em"></i> Main Menu</a>
          </div>
        </div>

        <div id="projects0_deetz" class="container-fluid py-3 px-3 shadow-lg text-center rounded-4 animate__animated animate__fadeIn animate__delay-1s" style="position: absolute; width: 40%; top: 10%; left: 42.5%; display: table-cell;">
          <h2>Projects Page</h2>
          <hr>
          <p align="justify">Click one of the links on the left to learn more about some of my projects.</p>
        </div>
        <div id="projects1_deetz" class="container-fluid py-3 px-3 shadow-lg text-center rounded-4 animate__animated animate__fadeIn animate__delay-1s overflow-scroll" style="position: absolute; width: 40%; top: 10%; left: 42.5%; display: table-cell;">
          <h2>ESRGAN Super-Resolution</h2>
          <hr>
          <div class="row">
            <div class="col">
              <p align="justify">The Enhanced Super-Resolution Generative Adversarial Network, or ESRGAN, is an improvement of standard Super-Resolution Generative Adversarial Networks (SRGAN) via introduction of the Residual-in-Residual Dense Block (RRDB). Essentially, the model takes an image and upscales it by a factor of 4. The results are much cleaner than when compared to traditional methods of upscaling images.</p>
              <p align="justify">The ESRGAN model was trained using images of size 128x128, so in order to upscale larger images, I divide the image into 128x128 tiles and process those with the model, and then reassemble into the larger image. You can view the project's GitHub page here:</p>
              <a class="btn btn-outline-dark me-3 shadow" href="https://github.com/alex-parisi/tfhub-esrgan-wrapper" role="button" target="_blank"><i class="bi bi-github"></i><br>Link</a>
            </div>
            <div class="col">
              <img src="esrgan_banner.jpg" class="img-fluid" alt="">
            </div>
          </div>
          <hr>
          <h4><u>Usage</u></h4>
          <p align="justify">First install the package with pip:</p>
          <div class="container-fluid" style="font-family:'Consolas'">pip install tfhub-esrgan-wrapper</div>
          <br>
          <p align="justify">Then, import the package:</p>
          <div class="container-fluid" style="font-family:'Consolas'">from tfhub_esrgan_wrapper import ESRGAN</div>
          <br>
          <p align="justify">To evaluate the model:</p>
          <div class="container-fluid" style="font-family:'Consolas'">esrgan = ESRGAN()<br>esrgan.load_image("input.jpg")<br>highres_image = esrgan.evaluate()</div>
          <br>
          <p align="justify">And to save the super-resolution output:</p>
          <div class="container-fluid" style="font-family:'Consolas'">from tfhub_esrgan_wrapper import save_image<br>save_image(highres_image, "output.jpg")</div>
          <br>
          <hr>
          <h4><u>Results</u></h4>
          <div class="row py-3">
            <div class="col">
              <img src="esrgan_results_input_1.jpg" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
              <p align="center"><em>Original<br>(1920 x 1080) - 1080p</em></p>
            </div>
            <div class="col">
              <img src="esrgan_results_output_1.jpg" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
              <p align="center"><em>Super-Resolution<br>(7680 x 4320) - 4K</em></p>
            </div>
          </div>
          <p align="justify">The image labeled "Original" is the input to the ESRGAN model, and the image labeled "Super-Resolution" is the output from the model. Click them to view the full-size images.</p>
          <br>
          <div class="row py-3">
            <div class="col">
              <img src="esrgan_results_input_2.jpg" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
              <p align="center"><em>Original</em></p>
            </div>
            <div class="col">
              <img src="esrgan_results_output_2.jpg" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
              <p align="center"><em>Super-Resolution</em></p>
            </div>
          </div>
          <p align="justify">The images above are a zoom-in of some ripples on the water. The results can be very impressive.</p>
          <br>
          <hr>
          <h4><u>JPEG Artifacts</u></h4>
          <div class="row py-3">
            <div class="col">
              <img src="little_me_input.jpg" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
              <p align="center"><em>Original</em></p>
            </div>
            <div class="col">
              <img src="little_me_output.jpg" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
              <p align="center"><em>Super-Resolution</em></p>
            </div>
          </div>
          <p align="justify">Although this image is an extreme example of JPEG compression, even artifacts that may be invisible to the naked eye (until closer inspection by zooming in on the image) can interfere with the ESRGAN model.</p>
          <p align="justify">While the ESRGAN model shows impressively results, it is not perfect. As shown above (me in highschool), if there are distortions or artifacts (like those typically present in JPEG compression), the ESRGAN model will only amplify those distortions. Because of this, it is imperative to work with images that contain as little distortions as possible.</p>
          <p align="justify">These distortions are typically a result of JPEG compression, which over time introduces more and more artifacts into an image. This is because JPEG compression is lossy, and not all of the original image information is preserved.</p>
          <p align="justify">Although the original picture was downloaded long ago, I can guess a couple of things: the original image was probably a screen shot of a sample displayed on a school photography website to avoid paying for it. After the original image was taken, it was probably compressed once for storage on the web-hosting service, and then is possibly compressed again when displayed on the webpage. When saving the screen shot as a JPEG, the image is compressed even further. All these layers of compression introduce significant distortions that the ESRGAN model only amplifies.</p>
          <br>
          <hr>
          <h4><u>Stitching Distortions</u></h4>
          <div class="row py-3">
            <div class="col">
              <img src="esrgan_stitching_1.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
              <p align="center"><em>Example 1</em></p>
            </div>
            <div class="col">
              <img src="esrgan_stitching_2.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
              <p align="center"><em>Example 2</em></p>
            </div>
          </div>
          <p align="justify">Because the ESRGAN model was trained on images of size 128x128, I convert the input image into separate 128x128 tiles to process individually by the model. Because of this, the borders of each tile will not align with its neighbors. This introduces what I will be calling stitching distortion.</p>
          <p align="justify">The two images above show examples of stitching distortion occuring on the border of four 128x128 tiles. The stitching distortion, like the distortions introduced in JPEG compression, are typically not visible until closer inspection via zoom-in.</p>
          <p align="justify">Currently, I can think of two ways to minimize or even eliminate these distortions:<br>1: Interpolate the regions near border intersections.<br>2. Increase the tile size in all directions during model evaluation, but then truncate the image again back to 128x128. This should give the model some information about the edges.</p>
          <br>
          <hr>
          <h4><u>How It Works</u></h4>
          <p align="justify">The ESRGAN model is a modification of the SSResNet generator network pictured below. The "Basic Block" used in the SSResNet network is modified by removing all batch-normalization layers, and then combining the remaining convolution layers into multi-level residual and dense connections.</p>
          <br>
          <img src="ssresnet.PNG" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <p align="center">ESRGAN Model</p>
          <br>
          <p align="justify">The image to the below shows a detail of the modified Basic Block, called the Residual-in-Residual Dense Block (RRDB). All of the Basic Blocks in the SSResNet are replaced with RRDB units.</p>
          <br>
          <img src="rrdb.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <p align="center">Residual-in-Residual Dense Block (RRDB)</p>
          <br>
          <p align="justify">The image below shows the network architecture for the discriminator - it follows the typical design standard for image classifiers and uses a variety of convolution and dense layers.</p>
          <br>
          <img src="ssresnet_discriminator.PNG" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <p align="center">Discriminator Architecture</p>
          <br>
          <p align="justify">Because the ESRGAN model is trained on images of size 128x128, any images larger than that are divided into tiles and processed individually. This process is what most likely is introducing the stitching distortion, so this method needs to be revisited.</p>
          <br>
          <hr>
          <h4><u>References</u></h4>
          <a class="link-dark" href="https://arxiv.org/pdf/1809.00219.pdf" target="_blank"><p align="left">Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, Xiaoou Tang. ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks. To appear in ECCV, 2018.</p></a>
          <a class="link-dark" href="https://arxiv.org/abs/1609.04802" target="_blank"><p align="left">Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi. Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. In CVPR, 2017.</p></a>
          <a class="link-dark" href="https://www.tensorflow.org/hub/tutorials/image_enhancing" target="_blank"><p align="left">https://www.tensorflow.org/hub/tutorials/image_enhancing</p></a>
        </div>
        <div id="projects2_deetz" class="container-fluid py-3 px-3 shadow-lg text-center rounded-4 animate__animated animate__fadeIn animate__delay-1s overflow-auto" style="position: absolute; width: 40%; top: 10%; left: 42.5%; display: table-cell;">
          <h2>Style Transfer</h2>
          <hr>
          <div class="row">
            <div class="col">
              <p align="justify">Pictured to the right is the result of applying the style of an abstract painting to a picture of a cute dog.</p>
              <p align="justify">I have written a Python package you can install with pip that encapsulates the model and associated evaluation functions into a simple wrapper. You can view the Github project page with the link below:</p>
              <a class="btn btn-outline-dark me-3 shadow" href="https://github.com/alex-parisi/tfhub-styletransfer-wrapper" role="button" target="_blank"><i class="bi bi-github"></i><br>Link</a>
            </div>
            <div class="col">
              <img src="style_transfer_banner.jpg" class="img-fluid" alt="">
            </div>
          </div>
          <hr>
          <h4><u>Usage</u></h4>
          <p align="justify">First install the package with pip:</p>
          <div class="container-fluid" style="font-family:'Consolas'">pip install tfhub-styletransfer-wrapper</div>
          <br>
          <p align="justify">Then, import the package:</p>
          <div class="container-fluid" style="font-family:'Consolas'">from tfhub_styletransfer_wrapper import StyleHub</div>
          <br>
          <p align="justify">Initiate the StyleHub module, and load the content and style images:</p>
          <div class="container-fluid" style="font-family:'Consolas'">stylehub = StyleHub()<br>stylehub.load_content(content_filename, 512)<br>stylehub.load_style(style_filename, 256)</div>
          <br>
          <p align="justify">To evaluate the model:</p>
          <div class="container-fluid" style="font-family:'Consolas'"> stylized_image = stylehub.evaluate()</div>
          <br>
          <p align="justify">And to save the stylized output:</p>
          <div class="container-fluid" style="font-family:'Consolas'">from tfhub_styletransfer_wrapper import save_image<br>save_image(stylized_image, output_filename) </div>
          <br>
          <hr>
          <h4><u>Results</u></h4>
          <img src="styletransfer-example1.jpg" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <img src="styletransfer-example2.jpg" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <img src="styletransfer-example3.jpg" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <img src="styletransfer-example4.jpg" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <br>
          <hr>
          <h4><u>How It Works</u></h4>
          <p align="justify">The image below shows the structure for the arbitrary style transfer model. The style prediction network P predicts an embedding vector S from an input style image, which supplies a set of normalization constants for the style transfer network.</p>
          <br>
          <img src="fast_style_transfer_arch-3.jpg" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <p align="center">Style Transfer Model</p>
          <br>
          <p align="justify">The style prediction network P is based on the Inception-v3 architecture and was developed as a way to scale up network architectures and avoid some of the increase in computational complexity that arises. Developed as an alternative to VGGNet, the Inception architecture offers improvements by avoiding representational bottlenecks, processing higher dimensional representations locally within a network (increasing the activations per tile), and factorizing large convolution filters into smaller ones.</p>
          <br>
          <img src="inceptionv3.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <p align="center">Inception-v3 Model</p>
          <br>
          <p align="justify">The Inception-v3 model (pictured above) is made up of both symmetric and asymmetric blocks like convolutions, average pooling, max pooling, concatinations, dropouts, and fully connected layers. Typically, each layer implements batch normalization, and the loss is computed using Softmax.</p>
          <p align="justify">The style transfer network T is a convolutional neural network formulated in the structure of an image encoder/decoder. The mean is computed across each channel of the Inception-v3 output S, which is then connected to two fully connected networks to predict the final embeddings. The overall effect of the style transfer network is that it will shift the mean and variance of the content image in an attempt to match the mean and variance of the style image's features.</p>
          <br>
          <hr>
          <h4><u>References</u></h4>
          <a class="link-dark" href="https://arxiv.org/abs/1705.06830" target="_blank"><p align="left">Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent Dumoulin, Jonathon Shlens. Exploring the structure of a real-time, arbitrary neural artistic stylization network. Proceedings of the British Machine Vision Conference (BMVC), 2017.</p></a>
          <a class="link-dark" href="https://arxiv.org/abs/1512.00567" target="_blank"><p align="left">Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna. Rethinking the Inception Architecture for Computer Vision. In IEEE Computer Vision and Pattern Recognition (CVPR), 2015.</p></a>
          <a class="link-dark" href="https://arxiv.org/abs/1703.06868" target="_blank"><p align="left">Xun Huang, Serge Belongie. Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization. In ICCV, 2017</p></a>
        </div>
        <div id="projects3_deetz" class="container-fluid py-3 px-3 shadow-lg text-center rounded-4 animate__animated animate__fadeIn animate__delay-1s overflow-auto" style="position: absolute; width: 40%; top: 10%; left: 42.5%; display: table-cell;">
          <h2>DCGAN</h2>
          <hr>
          <div class="row">
            <div class="col">
              <p align="justify">The image to the right describes the model architecture for the generator in a Deep Convolutional Generator Adversarial Network (DCGAN). This project implements a basic DCGAN for generating pictures of faces at a 64x64 resolution.</p>
              <p align="justify">The project was originally started as a standalone Python project, but facing incredibly long training times on my local machine, I looked into using Google Colab notebooks as they allow access to 8 TPU's for a short period of time. Using TPU's reduced the training time by a factor of 10.</p>
              <p align="justify">Over time, the network gets better at generating faces while also getting better at determing which faces are generated and which are from the original dataset. You can view the project's GitHub page here:</p>
              <a class="btn btn-outline-dark me-3 shadow" href="https://github.com/alex-parisi/DCGAN-Face-Generator/blob/main/DCGAN_Face_Generator.ipynb" role="button" target="_blank"><i class="bi bi-github"></i><br>Link</a>
            </div>
            <div class="col">
              <img src="dcgan_banner.png" class="img-fluid" alt="">
            </div>
          </div>
          <hr>
          <h4><u>Results</u></h4>
          <br>
          <div class="row">
            <div class="col">
              <img src="dcgan1.gif" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan2.gif" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan3.gif" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan4.gif" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan5.gif" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan6.gif" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan7.gif" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan8.gif" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan9.gif" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
          </div>
          <br>
          <p align="justify">The above GIFs show the network training over 100 epochs. Over time, the generator gets better at generating faces and the discriminator will get better at spotting fakes. Just taking a quick glance at the GIFs, it looks like the network begins to generate decent faces, and then quality begins to degrade - this is most likely due to overfitting and can possibly be mitigated by using a larger dataset.</p>
          <br>
          <div class="row">
            <div class="col">
              <img src="dcgan1_30.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan2_30.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan3_30.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan4_30.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan5_30.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan6_30.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan7_30.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan8_30.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan9_30.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
          </div>
          <br>
          <p align="justify">The images above are a snapshot of the network after 30 epochs of training. The network has already learned how to generate and discriminate basic facial structure and can even work with objects on the face such as sunglasses.</p>
          <br>
          <div class="row">
            <div class="col">
              <img src="dcgan1_60.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan2_60.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan3_60.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan4_60.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan5_60.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan6_60.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan7_60.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan8_60.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan9_60.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
          </div>
          <br>
          <p align="justify">The images above are a snapshot of the network after 60 epochs of training. While the network has gotten better at refining some facial detail, distortions are beginning to be introduced.</p>
          <br>
          <div class="row">
            <div class="col">
              <img src="dcgan1_90.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan2_90.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan3_90.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan4_90.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan5_90.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan6_90.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan7_90.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan8_90.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
            <div class="col">
              <img src="dcgan9_90.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
            </div>
          </div>
          <br>
          <p align="justify">The images above are a snapshot of the network after 90 epochs of training. The distortions have become very apparent.</p>
          <br>
          <hr>
          <h4><u>How It Works</u></h4>
          <p align="justify">The image below shows the overall structure of the DCGAN. A random vector the length of the latent space is generated and used as an input to the generator network, which then generates a fake image. The fake image and an image from the dataset are used as inputs to the discriminator network, which determines which image is from the original dataset and which is the fake.</p>
          <br>
          <img src="dcgan-model.jpg" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <p align="center">DCGAN Model</p>
          <br>
          <p align="justify">The dataset is the celebA dataset commonly used in facial recognition and face generation. It contains over 200,000 eye-aligned images of celebrity faces. To simplify this project and allow for reasonable training and evaluation times, the images are downsized to 64x64 and the network width is based off that. In order to use Google Colab, I had to host the celebA dataset on a Google Cloud Storage bucket - it currently has public access so the linked notebook should have access to it.</p>
          <br>
          <img src="generator.PNG" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <br>
          <p align="justify">As shown above, the generator takes an input vector of size 128 and uses a fully connected layer in conjunction with 4 convolution layers to upscale the latent space to the correct image size.</p>
          <p align="justify">The ReLU functions all use an alpha of 0.2 and the convolution layers use a kernel size of 5 and a stride of 2. The number of filters used in the convolution layers are 512, 256, 128, and 3 respectively. The fourth and final convolution layer also uses a sigmoid activation function.</p>
          <br>
          <img src="discriminator.PNG" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <br>
          <p align="justify">As shown above, the discriminator takes an input tensor of size (64, 64, 3) and uses 4 convolution filters in conjunction with a fully connected layer to extract features from the input and determine the probability that the input is real or fake.</p>
          <p align="justify">The ReLU functions all use an alpha of 0.2 and the convolution layers use a kernel size of 5 and a stride of 2. The number of filters used in the convolution layers are 128, 256, 512, and 1024 respectively. The dropout layer uses a dropout rate of 20%, and the fully connected layer uses a sigmoid activation function.</p>
          <br>
          <hr>
          <h4><u>References</u></h4>
          <a class="link-dark" href="https://developers.google.com/machine-learning/gan/generator" target="_blank"><p align="left">https://developers.google.com/machine-learning/gan/generator</p></a>
          <a class="link-dark" href="https://www.tensorflow.org/tutorials/generative/dcgan" target="_blank"><p align="left">https://www.tensorflow.org/tutorials/generative/dcgan</p></a>
          <a class="link-dark" href="https://keras.io/examples/generative/dcgan_overriding_train_step/" target="_blank"><p align="left">https://keras.io/examples/generative/dcgan_overriding_train_step/</p></a>
        </div>
        <div id="projects4_deetz" class="container-fluid py-3 px-3 shadow-lg text-center rounded-4 animate__animated animate__fadeIn animate__delay-1s overflow-auto" style="position: absolute; width: 40%; top: 10%; left: 42.5%; display: table-cell;">
          <h2>Django Website</h2>
          <hr>
          <p align="justify">Note: This project is no longer current, as I have opted to no longer deploy my website using Django. Using Heroku to host the app is starting to get expensive and using a static website tends to be a bit cheaper in this economic climate.</p>
          <hr>
          <div class="row">
            <div class="col">
              <p align="justify">This project details the deployment of this website using Django. The previous version of my website had been using static HTML pages, which are easy to create and relatively simple, and integrating web applications can be done using Flask. By using Django to deploy the website, I can easy deploy my web-apps, as well as programmatically create pages.</p>
              <p align="justify">The main attraction of using Django for my personal website was less of a necessity and more of a project. As I do not have hundreds of personal projects, using a database to store the project info is unnecessary, but still a fun exercise. It does allow me to more easily deploy my web-apps, as I have found Django to be much easier to use than integrating Flask.</p>
              <a class="btn btn-outline-dark me-3 shadow" href="https://github.com/alex-parisi/atparisi-django" role="button" target="_blank"><i class="bi bi-github"></i><br>Link</a>
            </div>
            <div class="col">
              <img src="django_banner.png" class="img-fluid" alt="">
            </div>
          </div>
          <br>
          <hr>
          <h4><u>Overview</u></h4>
          <br>
          <br>
          <img src="django_mvc.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <br>
          <br>
          <p align="justify">Django's web framework follows the Model-View-Controller (MVC) architectural pattern seen in the image above. Here is a high-level overview of how Django works:</p>
          <br>
          <p align="justify"><b>Models: </b>Python classes that represent the data structures of the application. These models define the database schema and handle interactions with the database.</p>
          <p align="justify"><b>Views: </b>handle the logic behind processing and responding to HTTP requests. A view receives an incoming request, performs necessary operations such as querying the database using models, and then returns an HTTP response.</p>
          <p align="justify"><b>Templates: </b>are used to generate HTML dynamically.</p>
          <p align="justify"><b>URLs: </b>are used to maps URLs to views. URL patterns are defined that specify URLs and their corresponding view functions.</p>
          <p align="justify"><b>Middleware: </b>are the components that sit between the web server and Django's view processing.</p>
          <br>
          <p align="justify">Django provides a high-level, Python-based framework that allows me to focus on building my web applications quickly and efficiently. It offers a wide range of built-in tools and libraries, simplifying common web development tasks and reducing the amount of code needed.</p>
          <p align="justify">Django follows the Don't Repeat Yourself (DRY) principle, promoting code reusability. It provides a modular structure with reusable components called apps, which can be easily plugged into different projects. This modular approach helps in scaling applications as they grow and simplifies maintenance.</p>
          <p align="justify"> There were many instances of copy-and-pasting code in between pages in my old static HTML website. For example, the Navbar at the top of the page can be imported into each page easily in Django, but requires it to be copy-and-pasted into every HTML file when using static pages. Also, any changes made to an application imported using Django easily cascades to all pages, avoiding having to make the same changes to all files individually.</p>
          <br>
          <hr>
          <h4><u>Templating</u></h4>
          <br>
          <br>
          <img src="templating.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <br>
          <br>
          <p align="justify">The general idea behind templating is to assemble an HTML webpage using different pieces, served by Django and populated with information from the database. For more simple requests like displaying a homepage, only some assembly is required.</p>
          <p align="justify">The <u>base.html</u> file is attached to every HTML page and serves as a base for the website - applications like the Navbar and information like the footer are stored here, and easily imported into each individual HTML page.</p>
          <p align="justify">The <u>&lt;project_name&gt;.html</u> file is attached to each project HTML page, and contains a more detailed breakdown of the project, installation of packages, and results.</p>
          <br>
          <hr>
          <h4><u>Database Structure</u></h4>
          <br>
          <img src="sqlite.jpg" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <br>
          <br>
          <p align="justify">High-level information about each project is stored in a database, so that it may be individually displayed on a general all-purpose Project Details template page. Each project has the following entries:</p>
          <div class="row">
            <div class="col">
              id
            </div>
            <div class="col">
              title
            </div>
            <div class="col">
              description
            </div>
            <div class="col">
              technology
            </div>
            <div class="col">
              image
            </div>
            <div class="col">
              details
            </div>
            <div class="col">
              link
            </div>
            <div class="col">
              project_contents
            </div>
            <div class="col">
              tags
            </div>
          </div>
          <br>
          <p align="justify">The <u>title</u> and <u>description</u> fields are used to populate the cards shown on the All Projects page.</p>
          <p align="justify">The <u>technology</u> field is not fully implemented yet, but will be tied into a search function for the project list.</p>
          <p align="justify">The <u>image</u> field is the image displayed in the project card, as well as the main banner image on the project detail page.</p>
          <p align="justify">The <u>details</u> field contains a short description of the project that is included at the top of each project detail page.</p>
          <p align="justify">The <u>link</u> field contains a link to the GitHub page for this project.</p>
          <p align="justify">The <u>project_content</u> field links to a separate HTML file that will ride along with the project detail page, contained a more detailed breakdown of how the project works and results.</p>
          <p align="justify">Finally, the <u>tags</u> field contains the relevant pages that are contained in the <u>project_content</u> HTML file. This allows for the mini-navbar to be generated at the bottom of the project summary.</p>
          <br>
          <hr>
          <h4><u>Static Files</u></h4>
          <br>
          <br>
          <img src="whitenoise.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <br>
          <br>
          <p align="justify">Typically, deploying a Django webpage into a live environment removes its ability serve static files, such as images. Instead of worrying about connecting to an EC2 instance and pulling the static files at runtime, I can instead use the <u>Whitenoise</u> middleware in Django. This allows me to still use static files, even when deployed, with the addition of only a few lines in my settings file.</p>
          <br>
          <hr>
          <h4><u>Deployment and Hosting</u></h4>
          <br>
          <br>
          <img src="heroku.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <br>
          <br>
          <p align="justify">The Django project is deployed using Heroku, as it allows for quick and easy deployment of Django projects, has easy integration for linking it to a domain, and is well-known across the industry. It also provides an easy way to set up SSL certificates and manage DNS settings, or route them elsewhere.</p>
          <p align="justify">For this project, I chose to use <u>CloudFlare</u> to manage my DNS settings, since Heroku and Google Domains don't play well with naked domain names (a naked domain is "atparisi.com", and not "www.atparisi.com") - Google Domains only allows A-records for naked domains, and Heroku will only allow ALIAS or ANAME records, which Google Domains does not offer. To use a naked domain properly in Heroku, we must use the ALIAS or ANAME configuration at the domain level, which unfortunately CloudFlare does not offer either; however, CloudFlare does offer a service called "CNAME Flattening", which allows for CNAME entries to be used at the domain level. This allows me to use a naked domain when deployed using Heroku and a Google Domain.</p>
          <p align="justify">Google Domains allows me to use a custom name server easily. All three services can be used together to deploy a live website in minutes, with very little configuration required.</p>
          <br>
        </div>
        <div id="projects5_deetz" class="container-fluid py-3 px-3 shadow-lg text-center rounded-4 animate__animated animate__fadeIn animate__delay-1s overflow-auto" style="position: absolute; width: 40%; top: 10%; left: 42.5%; display: table-cell;">
          <h2>OpenGL Renderer</h2>
          <hr>
          <div class="row">
            <div class="col">
              <p align="justify">This project details the development of a model renderer using OpenGL and GLFW.</p>
              <p align="justify">This renderer uses OpenGL version 3.3 which is very widely supported. GLAD is used to handle the locating of driver-specific functions, enabling interoperability between different systems. GLFW is used to provide easy functionality for creating an OpenGL instance, handling user input, and displaying our buffer to a window.</p>
              <a class="btn btn-outline-dark me-3 shadow" href="https://github.com/alex-parisi/OpenGL-Renderer" role="button" target="_blank"><i class="bi bi-github"></i><br>Link</a>
            </div>
            <div class="col">
              <img src="opengl_banner.gif" class="img-fluid" alt="">
            </div>
          </div>
          <br>
          <hr>
          <h4><u>Overview</u></h4>
          <br>
          <br>
          <img src="opengl_logo.jpg" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <br>
          <br>
          <p align="justify">This renderer uses <a class="link-dark" href="https://registry.khronos.org/OpenGL/specs/gl/glspec33.core.pdf" target="_blank">OpenGL</a> version 3.3 which is very widely supported. <a class="link-dark" href="https://github.com/Dav1dde/glad" target="_blank">GLAD</a> is used to handle the locating of driver-specific functions, enabling interoperability between different systems. <a class="link-dark" href="https://www.glfw.org/" target="_blank">GLFW</a> is used to provide easy functionality for creating an OpenGL instance, handling user input, and displaying our buffer to a window.</p>
          <br>
          <hr>
          <h4><u>Features</u></h4>
          <p align="justify">Geometric shape rendering<br>Model loading using Assimp<br>Directional and Point light sources<br>Blinn-Phon lighting model with linear gamma correction<br>Directional shadow mapping<br>Omnidirectional shadow maps<br>Normal mapping</p>
          <br>
          <hr>
          <h4><u>Structure</u></h4>
          <br>
          <br>
          <img src="opengl_structure.jpg" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <br>
          <br>
          <p align="justify">The engine handles the inputs from the keyboard and mouse, and therefore also handles the control of the camera. The camera and inputs are passed to the scene, which is rendered every frame. The scene contains a mixture of objects, models, and lights. In addition, there are various shaders attached to the scene that are used for different rendering conditions - like the generation of shadows.</p>
          <br>
          <hr>
          <h4><u>Shadow Mapping</u></h4>
          <br>
          <br>
          <img src="shadow_mapping.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <br>
          <br>
          <p align="justify">To generate both directional and point shadows, a shadow map is used - the scene is rendered from the point of view of the light source, and only the depth buffer is rendered. This allows each fragment to determine whether or not it is blocked from the light source.</p>
          <p align="justify">For directional light sources, an orthographic projection matrix is used to render the depth buffer of a scene. For point sources, a cube map is used instead.</p>
          <br>
          <hr>
          <h4><u>Model Loading</u></h4>
          <br>
          <br>
          <img src="assimp.png" class="img-fluid" alt="" onclick="window.open(this.src)" role="button">
          <br>
          <br>
          <p align="justify">Model loading is performed used <a class="link-dark" href="http://assimp.org/" target="_blank">Assimp</a>, which allows for the importing of models in a multitude of formats.</p>
          <br>
          <hr>
          <h4><u>References</u></h4>
          <a class="link-dark" href="https://learnopengl.com/" target="_blank"><p align="left">https://learnopengl.com/</p></a>
        </div>
      </div>

      <div id="the_linkz" class="container-fluid py-3 px-3 shadow-lg text-center rounded-4 animate__animated animate__fadeIn" style="position: absolute; width: 30%; bottom: 5%; left: 35%;">
        <div class="row py-3">
          <div class="col">
            <a id="resume_button" class="btn btn-outline-dark shadow animate__animated animate__fadeIn" style="animation-delay:1000ms" href="resume.pdf" role="button" target="_blank"><i class="bi bi-download"></i><br>Resume</a>
          </div>
          <div class="col">
            <a id="linkedin_button" class="btn btn-outline-dark shadow animate__animated animate__fadeIn" style="animation-delay:1250ms" href="https://www.linkedin.com/in/alextparisi/" role="button" target="_blank"><i class="bi bi-linkedin"></i><br>LinkedIn</a>
          </div>
          <div class="col">
            <a id="github_button" class="btn btn-outline-dark shadow animate__animated animate__fadeIn" style="animation-delay:1500ms" href="https://github.com/alex-parisi" role="button" target="_blank"><i class="bi bi-github"></i><br>GitHub</a>
          </div>
          <div class="col">
            <a id="email_button" class="btn btn-outline-dark shadow animate__animated animate__fadeIn" style="animation-delay:1750ms" href="mailto:alex@atparisi.com" role="button"><i class="bi bi-envelope"></i><br>Email</a>
          </div>
      </div>
    </div>
  </body>
</html>
